19.06

- Getting things done #project
    - Write down short introduction what we want. #sub5minutes
        - We want to create locally running, llm and neo4j based system, that will allow me to take notes as this file and they will be automatically processed by llm, ingested to knowledge graph. For interface we'll use streamlit, for backend we'll go with fastapi, for llm hosting we'll use ollama. For coding we're using VSCode branch Cursor, so I'm thinking about using devcontainters to contain all app elements. System will allow me to take notes, they will be highlighted, tagged automatically, you can follow the graph to get more context information. Dynamic system. What's also important - we want to create it in a way that will allow to use ollama or other providers as OpenAI, Google or other local system. #reference
    - Find on X post about "template" for Vibe Coding. #done
        - Here it is: https://pbs.twimg.com/media/GtfC0G8XQAAYJ-c?format=jpg&name=large #reference
        - I'll now paste this template to agent here and ask it to prepare.
            - We also found interesting vibe coding template, but we first need to do a security check over it. https://github.com/humanstack/vibe-coding-template #someday
        - It's on this way. We already have devcontainters and I'm checking whether they start.
        - Stub notebooks are not present, as notebooks can be created only with Claude models in Cursor. #done
            - Some Claude credits are in Pro plan of Cursor. #reference
    - Setting up ollama into my current devcontainer. #done
        - Must switch to 1.7b model due to low memory. Ollama code seems to be updated to Ollama 0.9.
        - Checking even 0.6b model.
    - We have a plan for today 20250619_plan-for-today.md #reference
    - Let's implement ingestion.py #done
        - First, neo4j client setup.
        - Neo is now connecting, things are added to it. Testing ollama ingestion script, it gives some troube.
        - Let's try to use think="false" flag to speed things up. Currently we're waiting very long for response. #next_task
            - It works. #reference
    - We can try tool-calling and streaming in the future #reference   
    - System is not doing what we want, it's based on separate added notes and I would like it to work on text I'm editing, processing each line, putting it into graph - with information about where line begins and ends - it would need to be updated if user changes the order of things. More like notepad it needs to be.
    - Changed to simpler workflow - for now we just take our note and ingest it into the graph with some llm enrichment. Tests are ongoing. #solvingtheissue
    - There's too much of this, so let's take two steps back and wrap it up.
        - Removing all bloat, leaving only notebooks and api, removing frontend, we'll use python scripts or notebook to recreate this pipeline and check what's happening under the hood. #next_task
            - As bloat I understand removing unneeded code and making sure we have no hardcoded values.
        - Try to add codebase knowledge base, generated automatically. #project
- Cursor notes
    - .cursorignore wrzucone do folderu powinno ignorować jego zawartość dla Cursora. #reference
- Prompts used #reference
    - Create a neo4j documentation for python ollama docker cli project working on automaticaly creating knowledge graphs from my semi structured notes, using llm to extract entities. Look for latest documentation. #done
        - For cursor project and Gemini Gem. Could be also used in ChatGPT project or Grok project.
        - Created first version with o4 in ChatGPT.
            - Failed to integrate MacOS ChatGPT application with Cursor running containers. #LLM_PROVIDER
            - Iterated with our codebase and asked for proposed changes. Request id: c16750a1-030b-4bcc-8c3e-b51a4720031a
                - Add batch processing to our ingesting pipeline. #project #someday
    - As above but for ollama 0.9.2 #next_task
        - Used same chat as previously for o4.
        - Got to work on neo4j again as it got removed by this promot.
    - And at the end I ended with the single documentation file containing all information. Probably because documentation was added to the context. Some kind of loop. #wonder
        - Anyway I'll check it against codebase and use anyway. I should try to redo the process for particular libraries documentations. #someday

