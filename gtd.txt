19.06

- Getting things done #project
    - Write down short introduction what we want. #sub5minutes
        - We want to create locally running, llm and neo4j based system, that will allow me to take notes as this file and they will be automatically processed by llm, ingested to knowledge graph. For interface we'll use streamlit, for backend we'll go with fastapi, for llm hosting we'll use ollama. For coding we're using VSCode branch Cursor, so I'm thinking about using devcontainters to contain all app elements. System will allow me to take notes, they will be highlighted, tagged automatically, you can follow the graph to get more context information. Dynamic system. What's also important - we want to create it in a way that will allow to use ollama or other providers as OpenAI, Google or other local system. #reference
    - Find on X post about "template" for Vibe Coding. #done
        - Here it is: https://pbs.twimg.com/media/GtfC0G8XQAAYJ-c?format=jpg&name=large #reference
        - I'll now paste this template to agent here and ask it to prepare.
            - We also found interesting vibe coding template, but we first need to do a security check over it. https://github.com/humanstack/vibe-coding-template #someday
        - It's on this way. We already have devcontainters and I'm checking whether they start.
        - Stub notebooks are not present, as notebooks can be created only with Claude models in Cursor. #done
            - Some Claude credits are in Pro plan of Cursor. #reference
    - Setting up ollama into my current devcontainer. #done
        - Must switch to 1.7b model due to low memory. Ollama code seems to be updated to Ollama 0.9.
        - Checking even 0.6b model.
    - We have a plan for today 20250619_plan-for-today.md #reference
    - Let's implement ingestion.py #done
        - First, neo4j client setup.
        - Neo is now connecting, things are added to it. Testing ollama ingestion script, it gives some troube.
        - Let's try to use think="false" flag to speed things up. Currently we're waiting very long for response. #next_task
            - It works. #reference
    - We can try tool-calling and streaming in the future #reference   
    - System is not doing what we want, it's based on separate added notes and I would like it to work on text I'm editing, processing each line, putting it into graph - with information about where line begins and ends - it would need to be updated if user changes the order of things. More like notepad it needs to be.
    - Changed to simpler workflow - for now we just take our note and ingest it into the graph with some llm enrichment. Tests are ongoing. #solvingtheissue
    - There's too much of this, so let's take two steps back and wrap it up.
        - Removing all bloat, leaving only notebooks and api, removing frontend, we'll use python scripts or notebook to recreate this pipeline and check what's happening under the hood. #done
            - As bloat I understand removing unneeded code and making sure we have no hardcoded values.
            - At the same time created 20250619_1846_Dev-notes-roadmap.md with more details about the future.
        - Try to add codebase knowledge base, generated automatically. #project
    - Move all hardcoded variables to config. #someday
    - Freeze requirements #next_task
    - Add Google API code to save some time with processing. After we get the basics we'll get to local models. #done
        - Gemini Pro 2.5 created the plan and we're trying to run it and fix at the same time.
            - Not good, not good :D Problems everywhere.
            - For now Gemini usage is hardcoded. For some reason Ollama is very persistent :D
            - Free Gemini API have 10RPM limit, that makes it quite unusable in this app. #reference
- Cursor notes
    - .cursorignore wrzucone do folderu powinno ignorować jego zawartość dla Cursora. #reference
- Prompts used #reference
    - Create a neo4j documentation for python ollama docker cli project working on automaticaly creating knowledge graphs from my semi structured notes, using llm to extract entities. Look for latest documentation. #done
        - For cursor project and Gemini Gem. Could be also used in ChatGPT project or Grok project.
        - Created first version with o4 in ChatGPT.
            - Failed to integrate MacOS ChatGPT application with Cursor running containers. #LLM_PROVIDER
            - Iterated with our codebase and asked for proposed changes. Request id: c16750a1-030b-4bcc-8c3e-b51a4720031a
                - Add batch processing to our ingesting pipeline. #project #someday
    - As above but for ollama 0.9.2 #failed
        - Used same chat as previously for o4.
        - Got to work on neo4j again as it got removed by this promot.
    - And at the end I ended with the single documentation file containing all information. Probably because documentation was added to the context. Some kind of loop. #wonder
        - Anyway I'll check it against codebase and use anyway. I should try to redo the process for particular libraries documentations. #someday
    - Asking Gemini Pro 2.5 to analyse my codebase, remove streamlit and all non used code and clean up with the files. #done

20.06

- Getting Things done.
    - We need to review, document and improve graph creation. #next_task
        - Created some tools to clean up knowledge graph.
    - We also need to review, document and improve the prompt we're using. #next_task
        - What's important - when creating LLM based description, we should take context into account to create better, hierarchical based descriptions.
        - During the graph improvement process, Cursor changed also our prompt.
    - Review what's happened yesterday and what should we try to do today. #next_task
    - For saving space purposes, we want to add option to disable and remove ollama container content. #done
        - Implemented Docker Compose profiles for optional Ollama service
        - Created cleanup script (scripts/cleanup-ollama.sh) to handle Ollama container cleanup
        - Updated documentation (README.md, techstack.md) with profile-based usage instructions
        - Created help script (scripts/help.sh) for quick command reference
        - System now supports: default mode (Neo4j only) and ollama profile mode (Neo4j + Ollama)
        - Cleanup script handles cases where docker-compose down doesn't fully stop Ollama containers
        - Forced removed the original devcontainer volume to save space.
    - On some day we can check tools that inspect containers and look for vulnerabilities. #someday
    - We need to start working on caching soon. Running the script over and over again takes quite a lot of time. We need to run llm only on lines that have changed. #next_task
        - There is a plan prepared by Gemini 2.5 Pro. We'll use is as guideline for Cursor agent and see what happens.
            - It completed the task. Now I need to run the pipeline to create hashes for the first time.
                - Initial scan conducted.
                    - One issue spotted - we must create some failsafe or retry mechanism, WARNING - Failed to parse LLM response as JSON for line 71, even after cleaning:  #known_issue
                    - Second issue is updating line numbers after line got added in the middle or removed. #known_issue
                    - Trying to solve this by more robust mechanism.
                        - And looks like it's working. We should #someday check it against more cases but for now it's working.
        - In case of parsing the lines, in the future we would need to check how it will work to use difs of new and old file. Find a way to keep track of changes. #someday
    - When adding information to the graph, we get warning about cartesian product. There's workaround and we should implement it sooner than later. #known_issue
    - Check if Bloom can be used in our current setup. If not, check if we can set up something on streamlit. #next_task
- Cursor
    - Check the GitHub integration with MCP. Idea is to use it to check repositories we want to use when creating knowledge graph out of our codebase. #someday
        - I tried to do it with Grok's assistance but failed. Got to try again.
    - Checking why docker ollama under devcontainers is using cpu only for serving models.
- Neo4j
    - Check if we can create different databases in current edition. #done
        - We can't, but we always can run another neo4j container. #reference


